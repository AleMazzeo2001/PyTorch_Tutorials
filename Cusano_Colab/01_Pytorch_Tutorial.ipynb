{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AleMazzeo2001/PyTorch_Tutorials/blob/main/01_Pytorch_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LABORARTORY OF MACHINE LEARNING"
      ],
      "metadata": {
        "id": "9OT1Fy-KTq-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Pytorch\n",
        "\n",
        "[Pytorch](https://pytorch.org/) is a free and open-source deep learning framework originally developed by Meta AI and now part of the Linux Foundation.\n",
        "Pytorch is widely used by the research community (while the industry may prefer [Tensorflow](https://www.tensorflow.org/))).\n",
        "The reference language is Python, but C++ and Java are also supported.\n",
        "Pytorch supports the main building blocks of a deep learning system: optimized numerical methods, automatic differentiation, and GPU computing.\n",
        "\n",
        "\n",
        "Topics:\n",
        "- Tensors\n",
        "- Autograd\n",
        "- Models\n",
        "- Data\n",
        "- Optimization\n",
        "\n",
        "\n",
        "To start using Pytorch in Colab, import it with the command:"
      ],
      "metadata": {
        "id": "WjlEPkErTxFF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-X16DMgHUxab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensors\n",
        "\n",
        "Pytorch stores data in **tensors** of class `torch.Tensor`. In this context, a tensor is a multi-dimensional array of homogenous numeric values.\n",
        "\n",
        "Pytorch tensors are similar to numpy arrays but with all the extra features needed to build deep learning systems (namely, automatic differentiation and GPU computing).\n",
        "\n",
        "You can create a tensor by calling the `torch.tensor' function and passing a list of values as argument."
      ],
      "metadata": {
        "id": "iCcsUocbVBAe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kf2hxt4sq2YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also create tensors by using one of the many factory functions."
      ],
      "metadata": {
        "id": "OdGTS4TGvPoy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hcg3eUWyx51g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two important properties of tensors are the shape and the data type."
      ],
      "metadata": {
        "id": "nro-FYR3zg7I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wWJsaDRvzgZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can change the shape and the dtype of tensors with the `view` and `to` methods (`view` creates a new tensor for the same data while `to` may create a copy)."
      ],
      "metadata": {
        "id": "LKjhDxF9z-09"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BxT9SMr1YLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors support the usual aritmetic operations `+`, `-`, `*`, `/`, `**`. There is also the `@` operator for vector inner product, and matrix multiplication."
      ],
      "metadata": {
        "id": "E6kD3kg34wBs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSJRHhPv6j_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pTensors also support reduction operators like `sum`, `mean`, `max`, `min`, `argmax`, `argmin`.\n",
        "\n",
        "They can work on all the values in the tensor, or along a given dimension."
      ],
      "metadata": {
        "id": "r46JJZlY_nuB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ox0OzEqJ_7-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A very useful mechanism in Pytorch is *broadcasting*. It allows to use together tensors of different shapes, provided that some conditions are fulfilled:\n",
        "- corresponding dimensions agree;\n",
        "- or when they disagree, at least one is exactly 1;\n",
        "- if the number of dimensions is different, dimensions of length one are added at the beginning of the shape until they match.\n",
        "\n",
        "Broadcasted dimensions are replicated.\n",
        "\n",
        "For instance:\n",
        "- shapes $(3, 4)$ and $(3, 4)$ are OK (identical);\n",
        "- shapes $(3, 1)$ and $(3, 4)$ are OK (the second dimension is broadcasted);\n",
        "- shapes $(3, 1)$ and $(1, 4)$ are OK (both dimensions are broadcasted);\n",
        "- shapes $(4,)$ and $(3, 4)$ are OK (the first dimension is added and broadcasted.\n",
        "\n",
        "Dimensions of length one can be removed with the `squeeze` method, and added with `unsqueeze`. The `T` attribute transposes 2D tensors."
      ],
      "metadata": {
        "id": "TCzrhS4WDinz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fsGRw49XIPp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Given the $4 \\times 2$ tensor `x` and the $5 \\times 2$ tensor `y`, each representing a set of vectors of length 2, compute the tensor $4 \\times 5$ tensor `M` representing the pairwise Euclidean distance between elements of `x` and `y`:\n",
        "\n",
        "$\\rm M[i,j] = \\| x[i, :] - y[j, :] \\|$\n",
        "\n",
        "Try to avoid loops, and use only basic operators and reduction methods (and the function `torch.sqrt` to compute the square root)."
      ],
      "metadata": {
        "id": "XIWyVeFEMR1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1.0, 2], [0, 1], [2, 2], [-1, 3]])\n",
        "y = torch.tensor([[0.0, -1], [1, 1], [2, 2], [0, 3], [-1, -1]])\n",
        "# Expected result\n",
        "# tensor([[3.1623, 1.0000, 1.0000, 1.4142, 3.6056],\n",
        "#         [2.0000, 1.0000, 2.2361, 2.0000, 2.2361],\n",
        "#         [3.6056, 1.4142, 0.0000, 2.2361, 4.2426],\n",
        "#         [4.1231, 2.8284, 3.1623, 1.0000, 4.0000]])\n"
      ],
      "metadata": {
        "id": "X5fm9mScNnmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autograd\n",
        "\n",
        "Autograd is the Pytorch component providing classes and functions for the automatic differentiation of arbitrary scalar valued functions. Typically you don't need to explicitly refer to autograd primitives. The other components of Pytorch will manage them in the right way unless special behavior is needed. Nevertheless, some knowledge od the basics of autograd can be useful.\n",
        "\n",
        "The `requires_grad` attribute of a tensor must be set to `True` to enable the automatic differentation mechanism. Pytorch will implicitly build a graph representing the computation involving these tensors. The `backward` method called on a scalar tensor will compute all the relevant derivatives and will store them in the `grad` attribute of the tensors.\n"
      ],
      "metadata": {
        "id": "sX-wqTqaZ4OE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3FKQu56cTRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d2a57a8-7bba-455a-bad0-7e92b1e7e546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(66.5500, grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradients need to be reset before starting a new computation."
      ],
      "metadata": {
        "id": "ugtRhzLGhdUc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XyhYOgO7heu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes it is useful to ignore how a tensor has been obtained, blocking the backward computation at that point. To do so, it is enough to call the `detach` mehots, which will return a new tensor for the same data, detached from the original graph."
      ],
      "metadata": {
        "id": "qWSuj_sjjOPx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXBnIxaujrgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to completely disable autograd from a sequence of operations, you can enclose them in the `torch.no_grad` context manager. This is often used to save computation after a model has been trained."
      ],
      "metadata": {
        "id": "5NrG7r_Qk7eu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3l8v_GZEk6QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Consider the function $L$ defined as:\n",
        "\n",
        "$L(y, z) = -y \\log p - (1 - y) \\log (1 - p), \\text{ where } p = \\frac{1}{1 + \\exp(-z)}.$\n",
        "\n",
        "Compute the value of $L$ in the following four cases:\n",
        "\n",
        "| z | y |\n",
        "|-|-|\n",
        "| -3 | 0 |\n",
        "| -1 | 0 |\n",
        "|  1.5 | 1 |\n",
        "|  4 | 1 |\n",
        "\n",
        "Then use autograd to compute the derivative of the average of $L$ with respect to the four values of $z$.\n",
        "\n",
        "The natural logarithm is computed by `torch.log`, and the exponential by `torch.exp`."
      ],
      "metadata": {
        "id": "uOAzeoJkQkKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.tensor([-3, -1, 1.5, 4])\n",
        "y = torch.tensor([0, 0, 1, 1])\n",
        "# Expected result:\n",
        "# 0.14535307884216309 (average L)\n",
        "# tensor([ 0.0119, 0.0672, -0.0456, -0.0045]) (gradient wrt z)\n"
      ],
      "metadata": {
        "id": "w3G3dp9MSBVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models\n",
        "\n",
        "In pytoch machine learning models are the composition of \"modules\". The most important class is `torch.nn.Module` which is the base for common ML operators and for their compositions.\n",
        "\n",
        "For instance, are subclasses of `torch.nn.Module` the following operations, widely used in the definition of neural networks:\n",
        "- `torch.nn.Linear`: fully connected layers;\n",
        "- `torch.nn.ReLU`, `torch.nn.Sigmoid`, and other activation functions;\n",
        "- `torch.nn.Conv`, `torch.nn.Conv2d`: convolutional layers;\n",
        "- `torch.nn.BatchNorm`, `torch.nn.InstanceNorm`, and other normalization functions;\n",
        "- `torch.nn.LSTM`, `torch.nn.GRU`: recurrent modules;\n",
        "- ...\n",
        "- and many others.\n",
        "\n",
        "The main feature of modules is the `forward` method, which takes as input a tensor, and computes a new tensor (some modules have multiple input and/or output tensors, but are not very common)."
      ],
      "metadata": {
        "id": "-ZYJ90z5KBDm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vLBsxou6N9J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A module stores its own parameters which, by default, are randomly initialized."
      ],
      "metadata": {
        "id": "KIe-aE8aOWDq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KoyKeNqQP3HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple models, consisting in a linear chain of modules can be defined using the `torch.nn.Sequential` class.  "
      ],
      "metadata": {
        "id": "2egiOuM0TjgI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hQkoVvlGTicf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For less simple models, where the application of modules is not strictly sequential, you must define your own `torch.nn.Module` subclass."
      ],
      "metadata": {
        "id": "k7UYjEHBUXxy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OFVHdAJlUoI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "Pytorch uses instances of `torch.utils.data.Dataset` to manage data.  Many popular datasets have been made available by the pytorch community. If you want to define your own, it is enogh to:\n",
        "1. define a new subclass of `torch.utils.data.Dataset`;\n",
        "2. implement a suitable `__init__` method;\n",
        "3. implement the `__len__` method, that returns the number of element in the dataset;\n",
        "4. implement the `__getitem__` method, that returns an element of the dataset, given its index.\n",
        "\n",
        "The elements in the dataset are typically tuples of tensors, or other kind of data."
      ],
      "metadata": {
        "id": "y_6gcfsYkuMa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yQMp33JGotR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Im many cases, you cannot afford to keep all the data in memory. The class `torch.utils.data.DataLoader` is pytorch's main solution to iterate over a dataset.\n",
        "\n",
        "Dataloaders provide several important features\n",
        "- they collate data into batches;\n",
        "- they randomly shuffle the data in the dataset;\n",
        "- they allows for parallel processing of the data by instantiating multiple workers (i.e., threads).\n",
        "\n",
        "To create a dataloder, just call the constructor, pass the dataset, and configure it by setting the many options it supports.\n",
        "To use the dataloader, just iterate over it with a python loop."
      ],
      "metadata": {
        "id": "5bkw7s1Gq7tV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EYlP-FJu18xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization\n",
        "\n",
        "Optimization is a key component of modern Machine Learning.\n",
        "Pytorch offers many optimization algorithms, based on gradient descent.\n",
        "\n",
        "Thanks to autograd, using them is very simple. First, you have to create one optimizer object, by selecting one of the many optimization algorithm. Then, in the main training loop, you must:\n",
        "\n",
        "1. reset the gradient by calling `optimizer.zero_grad()`;\n",
        "2. compute the loss function;\n",
        "3. compute the gradient by calling `loss.backward()`;\n",
        "4. take a step by calling `optimizer.step()`.\n"
      ],
      "metadata": {
        "id": "yCTbVHTp2ZXz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9TOkL3sk2ZBI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}