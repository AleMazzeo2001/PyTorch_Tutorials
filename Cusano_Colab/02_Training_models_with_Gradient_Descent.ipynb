{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AleMazzeo2001/PyTorch_Tutorials/blob/main/02_Training_models_with_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training models with Gradient Descent\n",
        "\n",
        "\n",
        "Thanks to automatic differentiation, pytorch makes it easy to implement and use gradient-based training algoerithms, like Gradient Descent.\n",
        "\n",
        "We will see this in action by training a logistic regression model on the \"exam\" dataset.\n",
        "\n",
        "Let's start by donwloading and visualizing the data."
      ],
      "metadata": {
        "id": "inhvIAzXbk_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://pastebin.com/raw/KTmF6b1u -O exam.txt\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "f = open('exam.txt', 'r')\n",
        "data = [float(s) for s in f.read().split()]\n",
        "f.close()\n",
        "\n",
        "XY = torch.tensor(data).view(-1, 3)\n",
        "X = XY[:, :2]\n",
        "Y = XY[:, 2].long()\n",
        "print(X.shape, X.dtype)\n",
        "print(Y.shape, Y.dtype)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap='rainbow')\n",
        "plt.xlabel('hours studying')\n",
        "plt.ylabel('hours attending lectures')"
      ],
      "metadata": {
        "id": "i1PUK-obj5yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Define the parameters and the function of the model. Remember that we need to compute:\n",
        "\n",
        "\n",
        "$$ z = w.T x + b, \\;\\; p = \\frac{1}{1 + e^{-z}}. $$\n",
        "\n",
        "Try to implement inference for multiple feature vectors in parallele, without using any loop.\n"
      ],
      "metadata": {
        "id": "mpHNa8iZAcWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logreg_inference(X, w, b):\n",
        "    # ... complete\n",
        "\n",
        "w = # ... complete\n",
        "b = # ... complete\n",
        "probs = logreg_inference(X, w, b)\n",
        "Yhat = (probs > 0.6).long()\n",
        "print(Yhat.shape)"
      ],
      "metadata": {
        "id": "IEpd0nBpnr82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Define the loss function, and the training loop.\n",
        "Recall the Gradient Descent update rule:\n",
        "$$ w \\gets w - \\eta \\nabla_w, L$$\n",
        "$$ b \\gets b - \\eta \\nabla_b. L$$\n",
        "\n",
        "For logistic regression we have:\\\n",
        "$$ \\nabla_w L = \\frac{1}{m} X^T (\\hat{p} - Y), $$\n",
        "$$ \\nabla_b L = \\frac{1}{m} \\sum_{i=0}^{m-1} (\\hat{p} - Y). $$\n",
        "\n",
        "The loss is the average cross entropy:\n",
        "$$ L = \\frac{1}{m}\\sum_{i=0}^{m-1} -Y_i \\log \\hat{p}_i -(1-Y_i) \\log (1 - \\hat{p}_i). $$\n"
      ],
      "metadata": {
        "id": "N9zvdTGxBQ5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STEPS = 100000\n",
        "LR = 0.002\n",
        "m = X.shape[0]\n",
        "\n",
        "def cross_entropy(Yhat, p):\n",
        "    # ... complete\n",
        "\n",
        "w = # ... complete\n",
        "b = # ... complete\n",
        "for step in range(STEPS):\n",
        "    p = # ... complete\n",
        "    loss = # ... complete\n",
        "    w_grad = # ... complete\n",
        "    b_grad = # ... commplete\n",
        "    w -= LR * w_grad\n",
        "    b -= LR * b_grad\n",
        "    if step % 1000 == 0:\n",
        "        Yhat = (p > 0.5).long()\n",
        "        accuracy = (Yhat == Y).float().mean()\n",
        "        print(step, loss.item(), accuracy.item())\n",
        "        steps.append(step)\n",
        "        losses.append(loss.item())\n",
        "        accuracies.append(accuracy.item())"
      ],
      "metadata": {
        "id": "U4zF8gLLzLMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modfy the code above to use automatic\n",
        "1. automatic differentiation;\n",
        "2. a pytorch optimizer."
      ],
      "metadata": {
        "id": "r3DjJQGblilX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the training loop to track the loss and the accuracy over the training iterations.\n",
        "\n",
        "plt.plot(steps, losses)\n",
        "plt.figure()\n",
        "plt.plot(steps, accuracies)"
      ],
      "metadata": {
        "id": "2m7jHS1gzL7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization\n",
        "\n",
        "This code shows the decision boundary of the classifier."
      ],
      "metadata": {
        "id": "tIp4LaluChV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1, x2 = torch.meshgrid(torch.linspace(0, 160, 100), torch.linspace(0, 70, 100))\n",
        "Xgrid = torch.stack([x1, x2], dim=2).reshape(-1, 2)\n",
        "p = logreg_inference(Xgrid, w, b).detach()\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap='rainbow')\n",
        "plt.contour(x1, x2, p.view(100, 100), levels=[0.25, 0.5, 0.75])"
      ],
      "metadata": {
        "id": "Pd2Y3JBY4AnI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}